# Data
data: 
    type: 'GrandChallenge' # {CIFAR-100,}
    tr: 
        path: '/ssd_data/aihub/product_train'
    test:
        path: '/ssd_data/aihub/product_val'
    augmentation:
        # {none, autoaugment }
        name: 'none'
    dali:
        avail: False

# Model
model: 
    # { resnet18, resnet50, resnet101, alexnet, squeezenet, vgg16, densenet, shufflenet, mobilenet, wide_resnet50_2, efficientnet-b0~-b7}
    # { micronet, micronet_pruned }
    # { resxnetv1-1.0x, resxnetv1-1.3x, resxnetv1-1.5x, resxnetv1-2.0x}
    name: 'rexnetv1_2.0x'  
    path: '/home/ikhee/workspace/Edge_Analytics/system/challenges/GrandChallenge/checkpoint'
    exp: 'focusmix'
    config: 'none'
    lr: 0.00125
    batch: 64 
    num_class: 245
    pretrained: True
    criterion: 
        name: 'cross_entropy' # {mse, cross_entropy, cutmix_cross_entropy, smooth_cross_entropy, label_smoothing }
        smoothing: 0.1
    # step 1: search basic micronet's hyperparameters
    architecture_search:
        type: 'none'  # {none, hyperopt}        
    # step 2: train
    training:
        type: 'train'  # { none, train, iterative_train }
        train_epochs: 100        # iterative_epochs = train_epochs // 4
    # step 3: prune
    pruning:
        type: 'none'    # {none, prune, iterative_prune }
        prune_iterations: 15     # Number of cycle of pruning that should be done
        prune_epochs: 200         # pruning epochs in each iteration
        prune_rate: 25.          # percentage of weight to be pruned after each cycle
        prune_target_ratio: 70   # percentage of target pruning
    # step 4: quantize
    quantization:
        type: 'none'   # {none, dorefa_net}
        quantize_epochs: 200
        wbits: 8
        abits: 8

# Regularization(model)
regularization:
    name: 'focusmix' # { None, cutout, mixup, cutmix, kdmix }
    beta: 1.0
    cutout:
        nholes: 1
        length: 16
    mixup:
    cutmix:
        prob: 0.0
    focusmix:
        wald_mean: 0.1
        wald_scale: 1.0
        lamda_min: 0.75
    ortho:
        avail: True
             
# Optimizer 
optimizer: 
    name: 'SGD'   # {Adadelta, Adagrad, Adam, AdamW, SparseAdam, Adamax, ASGD, LBFGS, RMSprop, Rprop, SGD }
                   # {AdamP, SGDP }
    weight_decay: 0.00001
    momentum: 0.9
    clip: 5

# Scheduler
scheduler: 
    name: 'CosineAnnealingLR'  # { CosineAnnealingLR, MultiStepLR }
    cosineannealinglr:
        # t_max: num_of_epochs
        eta_min: 0.0005
        last_epoch: -1
    multisteplr:
        gamma: 0.1
        milestones: [100,150]
    warmup:
        avail: False
        multiplier: 1 
        total_epoch: 5

# System(settings)
cuda: 
    avail: True
    device: 'cuda:2'   # { 'none' for cpu only, 'cuda:0' for GPU0, 'cuda;1' for GPU1, 'cuda' for all gpus }
verbose: True
